{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jgraph\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn import preprocessing\n",
    "import nltk\n",
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import keras\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dongwenjian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dongwenjian\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') # for tokenization\n",
    "nltk.download('stopwords')\n",
    "stpwds = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.stem.PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_classifier(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    df_recap= pd.DataFrame(np.zeros((2, 7)),columns=['LogReg', 'NaiveBayes','SVM', 'RandomForest', 'GBM', 'XGBoost', 'NNET'],\n",
    "                       index = ['F1', 'Accuracy'])\n",
    "\n",
    "\n",
    "    logit = LogisticRegression()\n",
    "    naiveb = GaussianNB()\n",
    "    svm_ = SVC()\n",
    "    rf = RandomForestClassifier()\n",
    "    gbm = GradientBoostingClassifier()\n",
    "    xg = xgb.XGBClassifier(max_depth=5, n_estimators=500, learning_rate=0.05)\n",
    "    nnet = MLPClassifier()\n",
    "   \n",
    "    classifiers= [logit, naiveb,svm_,rf,gbm ,xg, nnet]\n",
    "    idx=0\n",
    "    for classifier in classifiers: \n",
    "        \n",
    "        model = classifier.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)    \n",
    "        f1 = f1_score(y_test, predictions)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        \n",
    "        df_recap.iloc[0,idx]=np.round(f1,3)\n",
    "        df_recap.iloc[1,idx]=np.round(accuracy,3)\n",
    "        idx+=1\n",
    "    \n",
    "    return df_recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NodeSrc</th>\n",
       "      <th>NodeDest</th>\n",
       "      <th>Edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9510123</td>\n",
       "      <td>9502114</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9707075</td>\n",
       "      <td>9604178</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NodeSrc NodeDest Edge\n",
       "0  9510123  9502114    1\n",
       "1  9707075  9604178    1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Data/training_set.txt\", \"r\") as f:\n",
    "#with open(\"../Data/training_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    training_set  = list(reader)\n",
    "    \n",
    "training_set = [element[0].split(\" \") for element in training_set]\n",
    "training_set =  pd.DataFrame(training_set, columns=['NodeSrc', 'NodeDest', 'Edge'])\n",
    "training_set.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(training_set.NodeSrc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>year_pub</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>name_journal</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1001</td>\n",
       "      <td>2000</td>\n",
       "      <td>compactification geometry and duality</td>\n",
       "      <td>Paul S. Aspinwall</td>\n",
       "      <td></td>\n",
       "      <td>these are notes based on lectures given at tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002</td>\n",
       "      <td>2000</td>\n",
       "      <td>domain walls and massive gauged supergravity p...</td>\n",
       "      <td>M. Cvetic, H. Lu, C.N. Pope</td>\n",
       "      <td>Class.Quant.Grav.</td>\n",
       "      <td>we point out that massive gauged supergravity ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID year_pub                                              title  \\\n",
       "0  1001     2000              compactification geometry and duality   \n",
       "1  1002     2000  domain walls and massive gauged supergravity p...   \n",
       "\n",
       "                       authors       name_journal  \\\n",
       "0            Paul S. Aspinwall                      \n",
       "1  M. Cvetic, H. Lu, C.N. Pope  Class.Quant.Grav.   \n",
       "\n",
       "                                            abstract  \n",
       "0  these are notes based on lectures given at tas...  \n",
       "1  we point out that massive gauged supergravity ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Data/node_information.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    node_info  = list(reader)\n",
    "\n",
    "IDs = [element[0] for element in node_info]\n",
    "node_info =  pd.DataFrame(node_info, columns=['ID', 'year_pub', 'title','authors','name_journal','abstract'])\n",
    "node_info.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(node_info.iloc[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32648, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"Data/testing_set.txt\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    testing_set  = list(reader)\n",
    "\n",
    "testing_set = [element[0].split(\" \") for element in testing_set]\n",
    "testing_set =  pd.DataFrame(testing_set, columns=['NodeSrc', 'NodeDest'])\n",
    "testing_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly select 5% of training set to fit and validate the models \n",
    "# 95% of the remaining graph is used to create the graph features\n",
    "to_keep = random.sample(range(len(training_set)), k=int(round(len(training_set)*0.05)))\n",
    "dataset_train_val= training_set.iloc[to_keep]\n",
    "df_rest = training_set.loc[~training_set.index.isin(to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(584736, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30776, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes = df_rest.loc[df_rest['Edge']=='1']\n",
    "nodes= nodes[['NodeSrc','NodeDest' ]]\n",
    "nodes.to_csv('Data/nodes.txt', sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "DG=nx.DiGraph(directed=True)\n",
    "DG=nx.read_edgelist('Data/nodes.txt', create_using=nx.DiGraph(), nodetype = str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neighborsDict={}\n",
    "for n in DG.nodes():\n",
    "    neighborsDict[n]= list(DG.neighbors(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr = nx.pagerank(DG, alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_list = list(zip(dataset_train_val.NodeSrc, dataset_train_val.NodeDest))\n",
    "dict_pairs = pd.Series( my_list , index=dataset_train_val.index).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDPairs</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>comm_auth</th>\n",
       "      <th>cossim_a_tfidf</th>\n",
       "      <th>cossim_t_tfidf</th>\n",
       "      <th>lsa_abstract</th>\n",
       "      <th>lsa_title</th>\n",
       "      <th>nb_cit_indiv</th>\n",
       "      <th>common_out_neighbors</th>\n",
       "      <th>common_in_neighbors</th>\n",
       "      <th>jaccard_sim_out</th>\n",
       "      <th>jaccard_sim_in</th>\n",
       "      <th>shortest_path</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>node2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDPairs  overlap_title  overlap_abstract  temp_diff  comm_auth  \\\n",
       "0    39605            0.0               0.0        0.0        0.0   \n",
       "1   196908            0.0               0.0        0.0        0.0   \n",
       "\n",
       "   cossim_a_tfidf  cossim_t_tfidf  lsa_abstract  lsa_title  nb_cit_indiv  \\\n",
       "0             0.0             0.0           0.0        0.0           0.0   \n",
       "1             0.0             0.0           0.0        0.0           0.0   \n",
       "\n",
       "   common_out_neighbors  common_in_neighbors  jaccard_sim_out  jaccard_sim_in  \\\n",
       "0                   0.0                  0.0              0.0             0.0   \n",
       "1                   0.0                  0.0              0.0             0.0   \n",
       "\n",
       "   shortest_path  page_rank  node2vec  \n",
       "0            0.0        0.0       0.0  \n",
       "1            0.0        0.0       0.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= pd.DataFrame()\n",
    "dataset['IDPairs']= dict_pairs.keys()\n",
    "dataset['overlap_title'] = np.zeros(len(dataset))\n",
    "dataset['overlap_abstract'] = np.zeros(len(dataset))\n",
    "dataset['temp_diff'] = np.zeros(len(dataset))\n",
    "dataset['comm_auth'] = np.zeros(len(dataset))\n",
    "dataset['cossim_a_tfidf'] = np.zeros(len(dataset))\n",
    "dataset['cossim_t_tfidf'] = np.zeros(len(dataset))\n",
    "dataset['lsa_abstract'] = np.zeros(len(dataset))\n",
    "dataset['lsa_title'] = np.zeros(len(dataset))\n",
    "dataset['nb_cit_indiv'] = np.zeros(len(dataset)) \n",
    "\n",
    "dataset['common_out_neighbors'] = np.zeros(len(dataset))\n",
    "dataset['common_in_neighbors'] = np.zeros(len(dataset))\n",
    "dataset['jaccard_sim_out'] = np.zeros(len(dataset))\n",
    "dataset['jaccard_sim_in'] = np.zeros(len(dataset))\n",
    "dataset['shortest_path'] = np.zeros(len(dataset))\n",
    "dataset['page_rank'] = np.zeros(len(dataset))\n",
    "dataset['node2vec']   = np.zeros(len(dataset))\n",
    "\n",
    "\n",
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['center_distance_abstract'] =  np.zeros(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\ipykernel\\__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame.from_dict(dict_pairs, orient='index', columns=['NodeSrc','NodeDest'])\n",
    "temp['IDPairs']=dict_pairs.keys()\n",
    "dataset_train_val['IDPairs']=dataset_train_val.index\n",
    "df_merg=pd.merge(dataset_train_val, temp, on=['IDPairs'])\n",
    "df_merg=df_merg[['IDPairs','Edge']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDPairs</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>comm_auth</th>\n",
       "      <th>cossim_a_tfidf</th>\n",
       "      <th>cossim_t_tfidf</th>\n",
       "      <th>lsa_abstract</th>\n",
       "      <th>lsa_title</th>\n",
       "      <th>nb_cit_indiv</th>\n",
       "      <th>common_out_neighbors</th>\n",
       "      <th>common_in_neighbors</th>\n",
       "      <th>jaccard_sim_out</th>\n",
       "      <th>jaccard_sim_in</th>\n",
       "      <th>shortest_path</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>node2vec</th>\n",
       "      <th>Edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDPairs  overlap_title  overlap_abstract  temp_diff  comm_auth  \\\n",
       "0    39605            0.0               0.0        0.0        0.0   \n",
       "\n",
       "   cossim_a_tfidf  cossim_t_tfidf  lsa_abstract  lsa_title  nb_cit_indiv  \\\n",
       "0             0.0             0.0           0.0        0.0           0.0   \n",
       "\n",
       "   common_out_neighbors  common_in_neighbors  jaccard_sim_out  jaccard_sim_in  \\\n",
       "0                   0.0                  0.0              0.0             0.0   \n",
       "\n",
       "   shortest_path  page_rank  node2vec Edge  \n",
       "0            0.0        0.0       0.0    1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=pd.merge(dataset,df_merg, on='IDPairs')\n",
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDPairs</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>comm_auth</th>\n",
       "      <th>cossim_a_tfidf</th>\n",
       "      <th>cossim_t_tfidf</th>\n",
       "      <th>lsa_abstract</th>\n",
       "      <th>lsa_title</th>\n",
       "      <th>nb_cit_indiv</th>\n",
       "      <th>common_out_neighbors</th>\n",
       "      <th>common_in_neighbors</th>\n",
       "      <th>jaccard_sim_out</th>\n",
       "      <th>jaccard_sim_in</th>\n",
       "      <th>shortest_path</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>node2vec</th>\n",
       "      <th>Edge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208886</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578957</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDPairs  overlap_title  overlap_abstract  temp_diff  comm_auth  \\\n",
       "0    39605            0.0               0.0        0.0        0.0   \n",
       "1   196908            0.0               0.0        0.0        0.0   \n",
       "2   208886            0.0               0.0        0.0        0.0   \n",
       "3   578957            0.0               0.0        0.0        0.0   \n",
       "4    36851            0.0               0.0        0.0        0.0   \n",
       "\n",
       "   cossim_a_tfidf  cossim_t_tfidf  lsa_abstract  lsa_title  nb_cit_indiv  \\\n",
       "0             0.0             0.0           0.0        0.0           0.0   \n",
       "1             0.0             0.0           0.0        0.0           0.0   \n",
       "2             0.0             0.0           0.0        0.0           0.0   \n",
       "3             0.0             0.0           0.0        0.0           0.0   \n",
       "4             0.0             0.0           0.0        0.0           0.0   \n",
       "\n",
       "   common_out_neighbors  common_in_neighbors  jaccard_sim_out  jaccard_sim_in  \\\n",
       "0                   0.0                  0.0              0.0             0.0   \n",
       "1                   0.0                  0.0              0.0             0.0   \n",
       "2                   0.0                  0.0              0.0             0.0   \n",
       "3                   0.0                  0.0              0.0             0.0   \n",
       "4                   0.0                  0.0              0.0             0.0   \n",
       "\n",
       "   shortest_path  page_rank  node2vec Edge  \n",
       "0            0.0        0.0       0.0    1  \n",
       "1            0.0        0.0       0.0    0  \n",
       "2            0.0        0.0       0.0    1  \n",
       "3            0.0        0.0       0.0    0  \n",
       "4            0.0        0.0       0.0    1  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_list_test = list(zip(testing_set.NodeSrc, testing_set.NodeDest))\n",
    "dict_pairs_test = pd.Series(my_list_test, index=testing_set.index).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDPairs</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>comm_auth</th>\n",
       "      <th>cossim_a_tfidf</th>\n",
       "      <th>cossim_t_tfidf</th>\n",
       "      <th>lsa_abstract</th>\n",
       "      <th>lsa_title</th>\n",
       "      <th>nb_cit_indiv</th>\n",
       "      <th>common_out_neighbors</th>\n",
       "      <th>common_in_neighbors</th>\n",
       "      <th>jaccard_sim_out</th>\n",
       "      <th>jaccard_sim_in</th>\n",
       "      <th>shortest_path</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>node2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   IDPairs  overlap_title  overlap_abstract  temp_diff  comm_auth  \\\n",
       "0        0            0.0               0.0        0.0        0.0   \n",
       "1        1            0.0               0.0        0.0        0.0   \n",
       "\n",
       "   cossim_a_tfidf  cossim_t_tfidf  lsa_abstract  lsa_title  nb_cit_indiv  \\\n",
       "0             0.0             0.0           0.0        0.0           0.0   \n",
       "1             0.0             0.0           0.0        0.0           0.0   \n",
       "\n",
       "   common_out_neighbors  common_in_neighbors  jaccard_sim_out  jaccard_sim_in  \\\n",
       "0                   0.0                  0.0              0.0             0.0   \n",
       "1                   0.0                  0.0              0.0             0.0   \n",
       "\n",
       "   shortest_path  page_rank  node2vec  \n",
       "0            0.0        0.0       0.0  \n",
       "1            0.0        0.0       0.0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test= pd.DataFrame()\n",
    "dataset_test['IDPairs']= dict_pairs_test.keys()\n",
    "dataset_test['overlap_title'] = np.zeros(len(dataset_test))\n",
    "dataset_test['overlap_abstract'] = np.zeros(len(dataset_test))\n",
    "dataset_test['temp_diff'] = np.zeros(len(dataset_test))\n",
    "dataset_test['overlap_title'] = np.zeros(len(dataset_test))\n",
    "dataset_test['comm_auth'] = np.zeros(len(dataset_test))\n",
    "\n",
    "\n",
    "dataset_test['overlap_title'] = np.zeros(len(dataset_test))\n",
    "dataset_test['overlap_abstract'] = np.zeros(len(dataset_test))\n",
    "dataset_test['temp_diff'] = np.zeros(len(dataset_test))\n",
    "dataset_test['comm_auth'] = np.zeros(len(dataset_test))\n",
    "dataset_test['cossim_a_tfidf'] = np.zeros(len(dataset_test))\n",
    "dataset_test['cossim_t_tfidf'] = np.zeros(len(dataset_test))\n",
    "dataset_test['lsa_abstract'] = np.zeros(len(dataset_test))\n",
    "dataset_test['lsa_title'] = np.zeros(len(dataset_test))\n",
    "dataset_test['nb_cit_indiv'] = np.zeros(len(dataset_test)) \n",
    "\n",
    "dataset_test['common_out_neighbors'] = np.zeros(len(dataset_test))\n",
    "dataset_test['common_in_neighbors'] = np.zeros(len(dataset_test))\n",
    "dataset_test['jaccard_sim_out'] = np.zeros(len(dataset_test))\n",
    "dataset_test['jaccard_sim_in'] = np.zeros(len(dataset_test))\n",
    "dataset_test['shortest_path'] = np.zeros(len(dataset_test))\n",
    "dataset_test['page_rank'] = np.zeros(len(dataset_test))\n",
    "dataset_test['node2vec']   = np.zeros(len(dataset_test))\n",
    "\n",
    "dataset_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Text-based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number of overlapping words in the title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap_title(idPair, dict_pairs):\n",
    "    source = dict_pairs.get(idPair)[0]\n",
    "    target = dict_pairs.get(idPair)[1] \n",
    "    \n",
    "    source_info = node_info.loc [node_info['ID']==source]\n",
    "    target_info = node_info.loc [node_info['ID']==target]\n",
    "    \n",
    "    #Title\n",
    "    source_title = source_info.iloc[0,2].lower().split(\" \")   \n",
    "    source_title = [token for token in source_title if token not in stpwds]\n",
    "    source_title = [stemmer.stem(token) for token in source_title]\n",
    "      \n",
    "    target_title = source_info.iloc[0,2].lower().split(\" \")\n",
    "    target_title = [token for token in target_title if token not in stpwds]\n",
    "    target_title = [stemmer.stem(token) for token in target_title]\n",
    "    \n",
    "    return (len(set(source_title).intersection(set(target_title))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:31.985548\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset['overlap_title']=list(map(lambda i: overlap_title(i,dict_pairs), dataset['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:59.857006\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset_test['overlap_title']=list(map(lambda i: overlap_title(i,dict_pairs_test), dataset_test['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Number of overlapping words in the abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def overlap_abstract(idPair,dict_pairs):\n",
    "    source = dict_pairs.get(idPair)[0]\n",
    "    target = dict_pairs.get(idPair)[1] \n",
    "    \n",
    "    source_info = node_info.loc [node_info['ID']==source]\n",
    "    target_info = node_info.loc [node_info['ID']==target]\n",
    "    \n",
    "    #Abstract\n",
    "    source_abstract = source_info.iloc[0,5].lower().split(\" \")   \n",
    "    source_abstract = [token for token in source_abstract if token not in stpwds]\n",
    "    source_abstract = [stemmer.stem(token) for token in source_abstract]\n",
    "      \n",
    "    target_abstract = source_info.iloc[0,5].lower().split(\" \")\n",
    "    target_abstract = [token for token in target_abstract if token not in stpwds]\n",
    "    target_abstract = [stemmer.stem(token) for token in target_abstract]\n",
    "    \n",
    "    return (len(set(source_abstract).intersection(set(target_abstract))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-81bc7f6b20ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtime_beg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'overlap_abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moverlap_abstract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IDPairs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_beg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-116-81bc7f6b20ca>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtime_beg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'overlap_abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0moverlap_abstract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IDPairs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtime_beg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-a99db8118139>\u001b[0m in \u001b[0;36moverlap_abstract\u001b[0;34m(idPair, dict_pairs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msource_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msource_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msource_abstract\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstpwds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msource_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msource_abstract\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtarget_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-a99db8118139>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0msource_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msource_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msource_abstract\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstpwds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msource_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msource_abstract\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtarget_abstract\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msource_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step1a\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step1b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step1c\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_step1b\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m                 \u001b[1;34m'e'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m                 lambda stem: (self._measure(stem) == 1 and\n\u001b[0m\u001b[1;32m    377\u001b[0m                               self._ends_cvc(stem))\n\u001b[1;32m    378\u001b[0m             ),\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_apply_rule_list\u001b[0;34m(self, word, rules)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m             \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplacement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrule\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[1;32mif\u001b[0m \u001b[0msuffix\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'*d'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ends_double_consonant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m                 \u001b[0mstem\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcondition\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcondition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\nltk\\stem\\porter.py\u001b[0m in \u001b[0;36m_ends_double_consonant\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \"\"\"\n\u001b[1;32m    213\u001b[0m         return (\n\u001b[0;32m--> 214\u001b[0;31m             \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_consonant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         )\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset['overlap_abstract']=list(map(lambda i: overlap_abstract(i,dict_pairs), dataset['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:07:15.901143\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset_test['overlap_abstract']=list(map(lambda i: overlap_abstract(i,dict_pairs_test), dataset_test['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Temporal distance between the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tmp_dist(idPair,dict_pairs):\n",
    "    source = dict_pairs.get(idPair)[0]\n",
    "    target = dict_pairs.get(idPair)[1] \n",
    "    \n",
    "    source_info = node_info.loc [node_info['ID']==source]\n",
    "    target_info = node_info.loc [node_info['ID']==target]\n",
    "    \n",
    "    #Year\n",
    "    source_year = source_info.iloc[0,1]\n",
    "    target_year = target_info.iloc[0,1]\n",
    " \n",
    "    return (int(source_year) - int(target_year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:20.158177\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset['temp_diff']=list(map(lambda i: tmp_dist(i,dict_pairs), dataset['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:04:38.409913\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset_test['temp_diff']=list(map(lambda i: tmp_dist(i, dict_pairs_test), dataset_test['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def comm_auth(idPair, dict_pairs):\n",
    "    source = dict_pairs.get(idPair)[0]\n",
    "    target = dict_pairs.get(idPair)[1] \n",
    "    \n",
    "    source_info = node_info.loc [node_info['ID']==source]\n",
    "    target_info = node_info.loc [node_info['ID']==target]\n",
    "    \n",
    "    #Authors\n",
    "    source_auth = source_info.iloc[0,3].split(\",\")\n",
    "    target_auth = target_info.iloc[0,3].split(\",\")\n",
    " \n",
    "    return (len(set(source_auth).intersection(set(target_auth))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:12.986838\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset['comm_auth']=list(map(lambda i: comm_auth(i,dict_pairs), dataset['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:17.099837\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset_test['comm_auth']=list(map(lambda i: comm_auth(i, dict_pairs_test), dataset_test['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col= ['IDPairs','Edge']\n",
    "colnames=[i for i in dataset.columns if i not in col]\n",
    "X = dataset[colnames]\n",
    "\n",
    "print(\"Features : %s\"% list(X))\n",
    "\n",
    "y = dataset['Edge']\n",
    "y= list(map (lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "test_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Cosine distance - TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cosine distance between two TF-IDF abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models import LsiModel\n",
    "from gensim.similarities import MatrixSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_documents = list(node_info['abstract'])\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] \n",
    "            for text in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 22:20:39,334 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-01-06 22:20:41,305 : INFO : adding document #10000 to Dictionary(21076 unique tokens: ['2', 'a', 'and', 'are', 'as']...)\n",
      "2019-01-06 22:20:42,373 : INFO : adding document #20000 to Dictionary(30553 unique tokens: ['2', 'a', 'and', 'are', 'as']...)\n",
      "2019-01-06 22:20:43,514 : INFO : built Dictionary(35816 unique tokens: ['2', 'a', 'and', 'are', 'as']...) from 27770 documents (total 2819131 corpus positions)\n",
      "2019-01-06 22:20:48,114 : INFO : collecting document frequencies\n",
      "2019-01-06 22:20:48,115 : INFO : PROGRESS: processing document #0\n",
      "2019-01-06 22:20:48,306 : INFO : PROGRESS: processing document #10000\n",
      "2019-01-06 22:20:48,478 : INFO : PROGRESS: processing document #20000\n",
      "2019-01-06 22:20:48,616 : INFO : calculating IDF weights for 27770 documents and 35815 features (1811062 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "# Creation of dictionary, corpus and TF-IDF model\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "tf_idf = gensim.models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cossim_title(ID,dict_pairs,node_info,dictionary,tf_idf):\n",
    "    \n",
    "    doc1=node_info.loc[node_info['ID']==dict_pairs[ID][0],'title'].values[0]\n",
    "    doc2=node_info.loc[node_info['ID']==dict_pairs[ID][1],'title'].values[0]\n",
    "\n",
    "    idx_doc1=node_info.loc[node_info['ID']==dict_pairs[ID][0],'title'].index[0]\n",
    "    idx_doc2=node_info.loc[node_info['ID']==dict_pairs[ID][1],'title'].index[0]\n",
    "    \n",
    "    vec_bow1 = dictionary.doc2bow(doc1.lower().split())\n",
    "    vec_bow2 = dictionary.doc2bow(doc2.lower().split())\n",
    "\n",
    "    return gensim.matutils.cossim(tf_idf[vec_bow1], tf_idf[vec_bow2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.132623251279195 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "dataset['cossim_t_tfidf']=np.zeros(len(dataset))\n",
    "dataset['cossim_t_tfidf']=list(map(lambda i: cossim_title(i,dict_pairs,node_info,dictionary,tf_idf), dataset['IDPairs']))\n",
    "print(\"--- %s minutes ---\" % (np.float(time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 7.667019053300222 minutes ---\n"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "dataset_test['cossim_t_tfidf']=np.zeros(len(dataset_test))\n",
    "dataset_test['cossim_t_tfidf']=list(map(lambda i: cossim_title(i,dict_pairs_test,node_info,dictionary,tf_idf), dataset_test['IDPairs']))\n",
    "print(\"--- %s minutes ---\" % (np.float(time.time() - start_time)/60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col= ['IDPairs','Edge']\n",
    "colnames=[i for i in dataset.columns if i not in col]\n",
    "X = dataset[colnames]\n",
    "\n",
    "print(\"Features : %s\"% list(X))\n",
    "\n",
    "y = dataset['Edge']\n",
    "y= list(map (lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "test_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. The cosine distance between two LSA abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1059: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "# compute TFIDF vector of each paper\n",
    "corpus = set(node_info['abstract'])\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit LSA. Use algorithm = randomized for large datasets \n",
    "lsa = TruncatedSVD(100, algorithm = 'arpack')\n",
    "dtm_lsa = lsa.fit_transform(features_TFIDF)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T) \n",
    "save= pd.DataFrame(similarity,index=corpus, columns=corpus)\n",
    "save.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosdist_lsa_abstract(ID,dict_pairs ):    \n",
    "    abstract1=node_info.loc[node_info['ID']==dict_pairs[ID][0],'abstract'].values[0]\n",
    "    abstract2=node_info.loc[node_info['ID']==dict_pairs[ID][1],'abstract'].values[0]        \n",
    "    return save.loc[abstract1][abstract2]\n",
    "    ataset['lsa_abstract']=np.zeros(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset['lsa_abstract']=list(map(lambda i: cosdist_lsa_abstract(i, dict_pairs), dataset['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset_test['lsa_abstract']=list(map(lambda i: cosdist_lsa_abstract(i, dict_pairs_test), dataset_test['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. The cosine distance between two LSA titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute TFIDF vector of each paper\n",
    "corpus = set(node_info['title'])\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "features_TFIDF = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit LSA. Use algorithm = randomized for large datasets \n",
    "lsa = TruncatedSVD(100, algorithm = 'arpack')\n",
    "dtm_lsa = lsa.fit_transform(features_TFIDF)\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute document similarity using LSA components\n",
    "similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T) \n",
    "save= pd.DataFrame(similarity,index=corpus, columns=corpus)\n",
    "save.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosdist_lsa_title(ID, dict_pairs):    \n",
    "    title1=node_info.loc[node_info['ID']==dict_pairs[ID][0],'title'].values[0]\n",
    "    title2=node_info.loc[node_info['ID']==dict_pairs[ID][1],'title'].values[0]        \n",
    "    return save.loc[title1][title2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset['lsa_title']=list(map(lambda i: cosdist_lsa_title(i, dict_pairs), dataset['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset_test['lsa_title']=list(map(lambda i: cosdist_lsa_title(i, dict_pairs_test), dataset_test['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col= ['IDPairs','Edge']\n",
    "colnames=[i for i in dataset.columns if i not in col]\n",
    "X = dataset[colnames]\n",
    "\n",
    "print(\"Features : %s\"% list(X))\n",
    "\n",
    "y = dataset['Edge']\n",
    "y= list(map (lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "test_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Graph Based Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Number of citations between authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Common neighbours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Jaccard Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Shortest Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Page Rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Node2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './node2vec/emb/train_nodes_emb.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-7efd25ca4952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnode2vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[1;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./node2vec/emb/train_nodes_emb.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './node2vec/emb/train_nodes_emb.txt'"
     ]
    }
   ],
   "source": [
    "#nodes embeddings output from https://github.com/aditya-grover/node2vec\n",
    "#load node embedding\n",
    "\n",
    "node2vec = {}\n",
    "with io.open('./node2vec/emb/train_nodes_emb.txt', encoding='utf-8') as f:\n",
    "    next(f)\n",
    "    for i, line in enumerate(f):\n",
    "        node, vec = line.split(' ', 1)\n",
    "        node2vec[node] = np.fromstring(vec, sep=' ')\n",
    "print('Loaded %s pretrained node vectors' % (len(node2vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def n2v(ID, dict_pairs):\n",
    "    node_src= dict_pairs[ID][0]\n",
    "    nodes_dest= dict_pairs[ID][1]\n",
    "    if node_src not in node2vec or nodes_dest not in node2vec:\n",
    "        return 0\n",
    "        \n",
    "    v1 = node2vec[node_src] #w1 vector embedding\n",
    "    v2 = node2vec[nodes_dest] #w2 vector embedding\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset['node2vec']=list(map(lambda i: n2v(i,dict_pairs), dataset['IDPairs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_test['node2vec']=list(map(lambda i: n2v(i,dict_pairs_test), dataset_test['IDPairs']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col= ['IDPairs','Edge']\n",
    "colnames=[i for i in dataset.columns if i not in col]\n",
    "X = dataset[colnames]\n",
    "\n",
    "print(\"Features : %s\"% list(X))\n",
    "\n",
    "y = dataset['Edge']\n",
    "y= list(map(lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "test_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F. Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# imports needed and logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abstract_generator():\n",
    "    for abstract in node_info['abstract']:\n",
    "        yield gensim.utils.simple_preprocess(abstract)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 20:05:34,039 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-01-06 20:05:34,042 : INFO : collecting all words and their counts\n",
      "2019-01-06 20:05:34,044 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-06 20:05:34,256 : INFO : PROGRESS: at sentence #10000, processed 1004920 words, keeping 14716 word types\n",
      "2019-01-06 20:05:34,430 : INFO : PROGRESS: at sentence #20000, processed 1928403 words, keeping 20013 word types\n",
      "2019-01-06 20:05:34,575 : INFO : collected 22592 word types from a corpus of 2669524 raw words and 27770 sentences\n",
      "2019-01-06 20:05:34,576 : INFO : Loading a fresh vocabulary\n",
      "2019-01-06 20:05:34,644 : INFO : effective_min_count=2 retains 14598 unique words (64% of original 22592, drops 7994)\n",
      "2019-01-06 20:05:34,645 : INFO : effective_min_count=2 leaves 2661530 word corpus (99% of original 2669524, drops 7994)\n",
      "2019-01-06 20:05:34,712 : INFO : deleting the raw counts dictionary of 22592 items\n",
      "2019-01-06 20:05:34,714 : INFO : sample=0.001 downsamples 37 most-common words\n",
      "2019-01-06 20:05:34,715 : INFO : downsampling leaves estimated 1968327 word corpus (74.0% of prior 2661530)\n",
      "2019-01-06 20:05:34,834 : INFO : estimated required memory for 14598 words and 150 dimensions: 24816600 bytes\n",
      "2019-01-06 20:05:34,837 : INFO : resetting layer weights\n",
      "2019-01-06 20:05:35,105 : INFO : training model with 2 workers on 14598 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-06 20:05:36,142 : INFO : EPOCH 1 - PROGRESS: at 26.70% examples, 536920 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:37,144 : INFO : EPOCH 1 - PROGRESS: at 49.51% examples, 492152 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:38,147 : INFO : EPOCH 1 - PROGRESS: at 82.88% examples, 537385 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:38,660 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:05:38,662 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:05:38,663 : INFO : EPOCH - 1 : training on 2669524 raw words (1968482 effective words) took 3.5s, 555268 effective words/s\n",
      "2019-01-06 20:05:39,671 : INFO : EPOCH 2 - PROGRESS: at 32.39% examples, 665995 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:40,685 : INFO : EPOCH 2 - PROGRESS: at 64.97% examples, 640104 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:41,687 : INFO : EPOCH 2 - PROGRESS: at 97.94% examples, 638282 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:41,761 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:05:41,763 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:05:41,765 : INFO : EPOCH - 2 : training on 2669524 raw words (1968135 effective words) took 3.1s, 635604 effective words/s\n",
      "2019-01-06 20:05:42,774 : INFO : EPOCH 3 - PROGRESS: at 30.23% examples, 621325 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:43,777 : INFO : EPOCH 3 - PROGRESS: at 63.84% examples, 632483 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:44,782 : INFO : EPOCH 3 - PROGRESS: at 97.53% examples, 637496 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:44,858 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:05:44,866 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:05:44,867 : INFO : EPOCH - 3 : training on 2669524 raw words (1968453 effective words) took 3.1s, 635733 effective words/s\n",
      "2019-01-06 20:05:45,877 : INFO : EPOCH 4 - PROGRESS: at 31.72% examples, 650406 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:46,892 : INFO : EPOCH 4 - PROGRESS: at 66.82% examples, 657239 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:47,841 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:05:47,849 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:05:47,850 : INFO : EPOCH - 4 : training on 2669524 raw words (1968351 effective words) took 3.0s, 661173 effective words/s\n",
      "2019-01-06 20:05:48,893 : INFO : EPOCH 5 - PROGRESS: at 26.70% examples, 529794 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:49,897 : INFO : EPOCH 5 - PROGRESS: at 58.47% examples, 570845 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:50,918 : INFO : EPOCH 5 - PROGRESS: at 94.00% examples, 602713 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:05:51,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:05:51,106 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:05:51,107 : INFO : EPOCH - 5 : training on 2669524 raw words (1967698 effective words) took 3.3s, 605058 effective words/s\n",
      "2019-01-06 20:05:51,108 : INFO : training on a 13347620 raw words (9841119 effective words) took 16.0s, 614976 effective words/s\n"
     ]
    }
   ],
   "source": [
    "documents = list(abstract_generator())\n",
    "model = gensim.models.Word2Vec(\n",
    "        documents,\n",
    "        size=150,\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 20:06:29,653 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-01-06 20:06:29,656 : INFO : training model with 2 workers on 14598 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-06 20:06:30,664 : INFO : EPOCH 1 - PROGRESS: at 26.70% examples, 548994 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:31,665 : INFO : EPOCH 1 - PROGRESS: at 55.04% examples, 549526 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:32,670 : INFO : EPOCH 1 - PROGRESS: at 88.40% examples, 577714 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:33,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:33,027 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:33,028 : INFO : EPOCH - 1 : training on 2669524 raw words (1968529 effective words) took 3.4s, 584928 effective words/s\n",
      "2019-01-06 20:06:34,059 : INFO : EPOCH 2 - PROGRESS: at 25.35% examples, 507249 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:35,069 : INFO : EPOCH 2 - PROGRESS: at 58.47% examples, 573126 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:36,069 : INFO : EPOCH 2 - PROGRESS: at 93.27% examples, 603329 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:36,271 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:36,275 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:36,276 : INFO : EPOCH - 2 : training on 2669524 raw words (1967887 effective words) took 3.2s, 606728 effective words/s\n",
      "2019-01-06 20:06:37,300 : INFO : EPOCH 3 - PROGRESS: at 31.72% examples, 641908 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:38,301 : INFO : EPOCH 3 - PROGRESS: at 66.09% examples, 650130 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:39,270 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:39,278 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:39,280 : INFO : EPOCH - 3 : training on 2669524 raw words (1967926 effective words) took 3.0s, 656401 effective words/s\n",
      "2019-01-06 20:06:40,286 : INFO : EPOCH 4 - PROGRESS: at 23.26% examples, 475645 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:41,291 : INFO : EPOCH 4 - PROGRESS: at 57.70% examples, 573717 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:42,298 : INFO : EPOCH 4 - PROGRESS: at 92.88% examples, 605373 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:42,504 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:42,517 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:42,518 : INFO : EPOCH - 4 : training on 2669524 raw words (1968308 effective words) took 3.2s, 608739 effective words/s\n",
      "2019-01-06 20:06:43,530 : INFO : EPOCH 5 - PROGRESS: at 29.53% examples, 604520 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:44,534 : INFO : EPOCH 5 - PROGRESS: at 63.84% examples, 630925 words/s, in_qsize 4, out_qsize 1\n",
      "2019-01-06 20:06:45,555 : INFO : EPOCH 5 - PROGRESS: at 95.78% examples, 621403 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:45,691 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:45,699 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:45,700 : INFO : EPOCH - 5 : training on 2669524 raw words (1968954 effective words) took 3.2s, 619574 effective words/s\n",
      "2019-01-06 20:06:46,710 : INFO : EPOCH 6 - PROGRESS: at 32.06% examples, 658327 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:47,717 : INFO : EPOCH 6 - PROGRESS: at 63.84% examples, 630682 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:48,723 : INFO : EPOCH 6 - PROGRESS: at 97.53% examples, 636356 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:48,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:48,803 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:48,804 : INFO : EPOCH - 6 : training on 2669524 raw words (1968881 effective words) took 3.1s, 635333 effective words/s\n",
      "2019-01-06 20:06:49,810 : INFO : EPOCH 7 - PROGRESS: at 29.53% examples, 608123 words/s, in_qsize 4, out_qsize 0\n",
      "2019-01-06 20:06:50,813 : INFO : EPOCH 7 - PROGRESS: at 61.57% examples, 611149 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:51,818 : INFO : EPOCH 7 - PROGRESS: at 94.73% examples, 618470 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:52,002 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:52,006 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:52,008 : INFO : EPOCH - 7 : training on 2669524 raw words (1968210 effective words) took 3.2s, 615138 effective words/s\n",
      "2019-01-06 20:06:53,017 : INFO : EPOCH 8 - PROGRESS: at 19.14% examples, 386968 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:54,028 : INFO : EPOCH 8 - PROGRESS: at 43.15% examples, 437056 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:55,040 : INFO : EPOCH 8 - PROGRESS: at 78.58% examples, 511162 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:55,684 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:55,693 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:55,694 : INFO : EPOCH - 8 : training on 2669524 raw words (1968384 effective words) took 3.7s, 534758 effective words/s\n",
      "2019-01-06 20:06:56,701 : INFO : EPOCH 9 - PROGRESS: at 28.48% examples, 584273 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:57,716 : INFO : EPOCH 9 - PROGRESS: at 60.05% examples, 592028 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:58,721 : INFO : EPOCH 9 - PROGRESS: at 92.88% examples, 603433 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:06:58,939 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:06:58,950 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:06:58,951 : INFO : EPOCH - 9 : training on 2669524 raw words (1968154 effective words) took 3.3s, 604927 effective words/s\n",
      "2019-01-06 20:06:59,967 : INFO : EPOCH 10 - PROGRESS: at 31.72% examples, 645882 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:07:00,972 : INFO : EPOCH 10 - PROGRESS: at 64.61% examples, 636456 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:07:01,982 : INFO : EPOCH 10 - PROGRESS: at 89.89% examples, 583605 words/s, in_qsize 3, out_qsize 0\n",
      "2019-01-06 20:07:02,305 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 20:07:02,307 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 20:07:02,308 : INFO : EPOCH - 10 : training on 2669524 raw words (1968220 effective words) took 3.4s, 587116 effective words/s\n",
      "2019-01-06 20:07:02,310 : INFO : training on a 26695240 raw words (19683453 effective words) took 32.7s, 602826 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19683453, 26695240)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(documents, total_examples=len(documents), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.get_vector('solid').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_vectors = model.wv\n",
    "\n",
    "\n",
    "def centroid_distance_abstract(idPair, dict_pairs):\n",
    "    source = dict_pairs.get(idPair)[0]\n",
    "    target = dict_pairs.get(idPair)[1] \n",
    "    \n",
    "    source_info = node_info.loc[node_info['ID']==source]\n",
    "    target_info = node_info.loc[node_info['ID']==target]\n",
    "    \n",
    "    \n",
    "#     print(source_info['abstract'])\n",
    "#     #print(source_info['abstract'].to_string())\n",
    "#     print(len(source_info['abstract'].tolist()))\n",
    "#     print(source_info['abstract'].tolist()[0])\n",
    "#     for w in source_info['abstract'].tolist()[0].split():\n",
    "#         print(w)\n",
    "    \n",
    "    cen_source = np.zeros((word_vectors.vector_size,), dtype = 'float')\n",
    "    counter_source = 0\n",
    "    for w in source_info['abstract'].tolist()[0].split():\n",
    "        try:\n",
    "            cen_source += word_vectors.get_vector(w)\n",
    "            counter_source += 1  # If a word not exists, then this line doesn't run\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if counter_source == 0:  # If the node doesn't have a single valid word vector, return -1 as a flag.\n",
    "        return -1\n",
    "    cen_source /= counter_source\n",
    "    \n",
    "    cen_target = np.zeros((word_vectors.vector_size,), dtype = 'float')\n",
    "    counter_target = 0\n",
    "    for w in target_info['abstract'].tolist()[0].split():\n",
    "        try:\n",
    "            cen_target += word_vectors.get_vector(w)\n",
    "            counter_target += 1\n",
    "        except KeyError:\n",
    "                pass\n",
    "    if counter_target == 0:\n",
    "        return -1\n",
    "    cen_target /= counter_target\n",
    "    \n",
    "    return np.linalg.norm(cen_source - cen_target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.424449203422787"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroid_distance_abstract(39605, dict_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timefo_beg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-9d7eec5042a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'center_distance_abstract'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcentroid_distance_abstract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdict_pairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'IDPairs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtime_end\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime_end\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtimefo_beg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'timefo_beg' is not defined"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset['center_distance_abstract']=list(map(lambda i: centroid_distance_abstract(i,dict_pairs), dataset['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDPairs</th>\n",
       "      <th>overlap_title</th>\n",
       "      <th>overlap_abstract</th>\n",
       "      <th>temp_diff</th>\n",
       "      <th>comm_auth</th>\n",
       "      <th>cossim_a_tfidf</th>\n",
       "      <th>cossim_t_tfidf</th>\n",
       "      <th>lsa_abstract</th>\n",
       "      <th>lsa_title</th>\n",
       "      <th>nb_cit_indiv</th>\n",
       "      <th>common_out_neighbors</th>\n",
       "      <th>common_in_neighbors</th>\n",
       "      <th>jaccard_sim_out</th>\n",
       "      <th>jaccard_sim_in</th>\n",
       "      <th>shortest_path</th>\n",
       "      <th>page_rank</th>\n",
       "      <th>node2vec</th>\n",
       "      <th>Edge</th>\n",
       "      <th>center_distance_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39605</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.424449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>196908</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.207750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>208886</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.419310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578957</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.042357</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.174467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36851</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.204469</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.413794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>160011</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.595990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.620734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>50043</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.150132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.801676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>260639</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.616259</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.465783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>460072</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.058379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>324093</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.218448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>464275</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.921777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>265698</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.365143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    IDPairs  overlap_title  overlap_abstract  temp_diff  comm_auth  \\\n",
       "0     39605              7               0.0          1          0   \n",
       "1    196908              3               0.0         -4          0   \n",
       "2    208886              5               0.0          1          0   \n",
       "3    578957              5               0.0          7          0   \n",
       "4     36851              6               0.0          0          0   \n",
       "5    160011              7               0.0          5          0   \n",
       "6     50043              6               0.0         -2          0   \n",
       "7    260639              3               0.0          1          0   \n",
       "8    460072              6               0.0         -2          0   \n",
       "9    324093              2               0.0          1          0   \n",
       "10   464275              5               0.0          3          0   \n",
       "11   265698              4               0.0          5          0   \n",
       "\n",
       "    cossim_a_tfidf  cossim_t_tfidf  lsa_abstract  lsa_title  nb_cit_indiv  \\\n",
       "0              0.0        0.076979           0.0        0.0           0.0   \n",
       "1              0.0        0.000000           0.0        0.0           0.0   \n",
       "2              0.0        0.000002           0.0        0.0           0.0   \n",
       "3              0.0        0.042357           0.0        0.0           0.0   \n",
       "4              0.0        0.204469           0.0        0.0           0.0   \n",
       "5              0.0        0.595990           0.0        0.0           0.0   \n",
       "6              0.0        0.150132           0.0        0.0           0.0   \n",
       "7              0.0        0.616259           0.0        0.0           0.0   \n",
       "8              0.0        0.000013           0.0        0.0           0.0   \n",
       "9              0.0        0.191881           0.0        0.0           0.0   \n",
       "10             0.0        0.000000           0.0        0.0           0.0   \n",
       "11             0.0        0.000000           0.0        0.0           0.0   \n",
       "\n",
       "    common_out_neighbors  common_in_neighbors  jaccard_sim_out  \\\n",
       "0                    0.0                  0.0              0.0   \n",
       "1                    0.0                  0.0              0.0   \n",
       "2                    0.0                  0.0              0.0   \n",
       "3                    0.0                  0.0              0.0   \n",
       "4                    0.0                  0.0              0.0   \n",
       "5                    0.0                  0.0              0.0   \n",
       "6                    0.0                  0.0              0.0   \n",
       "7                    0.0                  0.0              0.0   \n",
       "8                    0.0                  0.0              0.0   \n",
       "9                    0.0                  0.0              0.0   \n",
       "10                   0.0                  0.0              0.0   \n",
       "11                   0.0                  0.0              0.0   \n",
       "\n",
       "    jaccard_sim_in  shortest_path  page_rank  node2vec Edge  \\\n",
       "0              0.0            0.0        0.0       0.0    1   \n",
       "1              0.0            0.0        0.0       0.0    0   \n",
       "2              0.0            0.0        0.0       0.0    1   \n",
       "3              0.0            0.0        0.0       0.0    0   \n",
       "4              0.0            0.0        0.0       0.0    1   \n",
       "5              0.0            0.0        0.0       0.0    1   \n",
       "6              0.0            0.0        0.0       0.0    0   \n",
       "7              0.0            0.0        0.0       0.0    1   \n",
       "8              0.0            0.0        0.0       0.0    0   \n",
       "9              0.0            0.0        0.0       0.0    1   \n",
       "10             0.0            0.0        0.0       0.0    1   \n",
       "11             0.0            0.0        0.0       0.0    0   \n",
       "\n",
       "    center_distance_abstract  \n",
       "0                   3.424449  \n",
       "1                   4.207750  \n",
       "2                   3.419310  \n",
       "3                   4.174467  \n",
       "4                   2.413794  \n",
       "5                   2.620734  \n",
       "6                   3.801676  \n",
       "7                   2.465783  \n",
       "8                   6.058379  \n",
       "9                   4.218448  \n",
       "10                  4.921777  \n",
       "11                  4.365143  "
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:05:10.372880\n"
     ]
    }
   ],
   "source": [
    "time_beg = datetime.datetime.now()\n",
    "dataset_test['center_distance_abstract']=list(map(lambda i: centroid_distance_abstract(i,dict_pairs_test), dataset_test['IDPairs']))\n",
    "time_end = datetime.datetime.now()\n",
    "print (time_end-time_beg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : ['overlap_title', 'overlap_abstract', 'temp_diff', 'comm_auth', 'cossim_a_tfidf', 'cossim_t_tfidf', 'lsa_abstract', 'lsa_title', 'nb_cit_indiv', 'common_out_neighbors', 'common_in_neighbors', 'jaccard_sim_out', 'jaccard_sim_in', 'shortest_path', 'page_rank', 'node2vec', 'center_distance_abstract']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogReg</th>\n",
       "      <th>NaiveBayes</th>\n",
       "      <th>SVM</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>GBM</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>NNET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.785</td>\n",
       "      <td>0.558</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.834</td>\n",
       "      <td>0.831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.763</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.816</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LogReg  NaiveBayes    SVM  RandomForest    GBM  XGBoost   NNET\n",
       "F1         0.785       0.558  0.834         0.798  0.838    0.834  0.831\n",
       "Accuracy   0.763       0.654  0.809         0.784  0.821    0.815  0.816"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col= ['IDPairs','Edge']\n",
    "colnames=[i for i in dataset.columns if i not in col]\n",
    "X = dataset[colnames]\n",
    "\n",
    "print(\"Features : %s\"% list(X))\n",
    "\n",
    "y = dataset['Edge']\n",
    "y= list(map (lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "test_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features : ['overlap_title', 'overlap_abstract', 'temp_diff', 'comm_auth', 'cossim_a_tfidf', 'cossim_t_tfidf', 'lsa_abstract', 'lsa_title', 'nb_cit_indiv', 'common_out_neighbors', 'common_in_neighbors', 'jaccard_sim_out', 'jaccard_sim_in', 'shortest_path', 'page_rank', 'node2vec']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\sklearn\\preprocessing\\label.py:171: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LogReg</th>\n",
       "      <th>NaiveBayes</th>\n",
       "      <th>SVM</th>\n",
       "      <th>RandomForest</th>\n",
       "      <th>GBM</th>\n",
       "      <th>XGBoost</th>\n",
       "      <th>NNET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.667</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.779</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.811</td>\n",
       "      <td>0.810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.654</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.765</td>\n",
       "      <td>0.748</td>\n",
       "      <td>0.775</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          LogReg  NaiveBayes    SVM  RandomForest    GBM  XGBoost   NNET\n",
       "F1         0.667       0.549  0.811         0.779  0.810    0.811  0.810\n",
       "Accuracy   0.654       0.650  0.765         0.748  0.775    0.776  0.775"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col= ['IDPairs','Edge','center_distance_abstract']\n",
    "colnames=[i for i in dataset.columns if i not in col]\n",
    "X = dataset[colnames]\n",
    "\n",
    "print(\"Features : %s\"% list(X))\n",
    "\n",
    "y = dataset['Edge']\n",
    "y= list(map (lambda i: int(i), y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "test_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "model_rf = rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame()\n",
    "features['feature'] = X.columns\n",
    "features['importance'] = model_rf.feature_importances_\n",
    "features.sort_values(by=['importance'], ascending=False, inplace=True)\n",
    "features.set_index('feature', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(features.values.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importance = list(features.values.reshape(-1))\n",
    "bars = list(features.index.values)\n",
    "\n",
    "y_pos = np.arange(len(bars))\n",
    " \n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "# Create horizontal bars\n",
    "plt.barh(y_pos, importance)\n",
    " \n",
    "# Create names on the y-axis\n",
    "plt.yticks(y_pos, bars)\n",
    "\n",
    "# Add title\n",
    "plt.title('Feature Importance')\n",
    "\n",
    "# Show graphic\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tps0=time.clock()\n",
    "\n",
    "# Classifiers\n",
    "rf   = RandomForestClassifier()\n",
    "gbm = GradientBoostingClassifier()\n",
    "boost = xgb.XGBClassifier()\n",
    "svm_  = SVC()\n",
    "logi = LogisticRegression()\n",
    "nnet=MLPClassifier()\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Number of iterations\n",
    "B=10 # to test the loop : use B=3 instead\n",
    "\n",
    "# Parameters grids\n",
    "\n",
    "listMethGrid=[logi,nb, svm_,rf,gbm,boost,nnet]\n",
    "arrayErreur=np.empty((B,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(B):\n",
    "    print(i)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)\n",
    "    \n",
    "    # Computation of test error\n",
    "    for j,method in enumerate(listMethGrid):\n",
    "        methFit=method.fit(X_train, y_train)\n",
    "        predictions = methFit.predict(X_test)\n",
    "        arrayErreur[i,j]=f1_score(y_test, predictions)\n",
    "        \n",
    "tps1=time.clock()\n",
    "print(\"Execution time in mn :\",(tps1 - tps0)/60)\n",
    "\n",
    "dataframeErreur=pd.DataFrame(arrayErreur,columns=[\"Logit\",\"NB\",\"SVM\",\"RF\",\"GBM\",\"XGB\",\"NNET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, ax = plt.subplots(figsize=(16,9))\n",
    "x = list(dataframeErreur.columns.values) * 10\n",
    "y = list(dataframeErreur.values.reshape(-1))\n",
    "sns.boxplot(x=x, y=y, linewidth=2)\n",
    "plt.title(\"F1 Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col= ['IDPairs','Edge']\n",
    "colnames=[i for i in dataset.columns if i not in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier()\n",
    "model = model.fit(X_train, y_train)\n",
    "test=dataset_test[colnames]\n",
    "predictions = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_pred = pd.concat([dataset_test['IDPairs'],pd.DataFrame(predictions)],axis=1)\n",
    "final_pred.columns = ['id','category']\n",
    "final_pred.to_csv('../Data/my_pred.csv',index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.to_csv('Data/dataset.csv', sep=',')\n",
    "dataset_test.to_csv('Data/dataset_test.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
